%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
\vspace*{-.1cm}
\section{OpenStack at the Edge}
\label{sec:system_design_considerations}
\vspace*{-.1cm}

%\AL{We should add one sentence that justify why we have this section}
%
This section studies the use of OpenStack to control an \edge
infrastructure. Such an analysis is meaningful as the OpenStack
codebase has been evolving to deal with large scale and multi-site
objectives for the two last years. Particularly, the section evaluates
OpenStack, including its latest improvements, with respect to the requirements
defined in the previous section.
%in
%regards to the requirements defined in the previous section,
%highlighting why changes are mandatory despite recent improvements.


At a high level, OpenStack has two types of nodes: data nodes
delivering XaaS capabilities (compute/storage/network, \ie data
plane); control nodes executing OpenStack services (\ie control
plane). Whenever users issue a request to OpenStack, the control
plane processes the request which may potentially also affect the data plane in some manner. 
% which ends on the data plane. 
Key control plane services include \verb|keystone|,
\verb|nova|, \verb|glance|, and \verb|neutron|, respectively responsible for 
authentication/authorization, VM life cycle management, VM image management and
associated network management.

Because OpenStack comes with several deployment alternatives and
because the \edge sites considered in our study can host data and
control nodes, we elected to discuss two scenarios:
a centralized management scenario and a multiple regions scenario described next.
% The first one tries to mimic a
%single DC environment by having a single OpenStack with all control
%services on one site and data nodes on the other sites.  The second
%corresponds to a multi-region deployment, where one OpenStack
%is deployed per edge site.

\vspace*{-.3cm}
\subsection{Centralized (Remote) Management}
\label{subsec:centralized_os}
In this scenario, OpenStack operates an \edge infrastructure
as a traditional single DC environment, %. %Cloud Computing one.
the key difference being the wide-area networking found between the
control and compute nodes~\cite{www:openstack-wanwide}. The
distinction between the different \edge sites can be realized by leveraging the
concept of host aggregates within OpenStack.

From the requirements' viewpoint, L1 and most L2 requirements can be
fulfilled in a straightforward way (because the infrastructure can be
spread over several network domains, some L2 operations including
specific network actions cannot be satisfied). For L3, only L3.1
requirements can be satisfied while L3.2 cannot be met due to
the centralized control plane.
For instance, most OpenStack services collaborate through a message system and
through the manipulation of logical objects that are persisted in shared
databases (DBs). While this enables services to easily collaborate, it imposes
a requirement of permanent connectivity between the services located at the
compute nodes and the control services like the databases and message buses
located in the DC.
In other words, while this scenario provides a ``Single Pane of
Glass'' for administrators and DevOps, it has the drawback of being a ``Single
Point of Failure'' preventing DevOps to use \edge resources in case of network
split-brains.

\vspace*{-.2cm}
\subsection{Multiple Regions}
In this scenario, each \edge site corresponds to a \emph{region}
in the OpenStack terminology, which is a complete deployment of
OpenStack with all control services and a ``shared'' Keystone.
% (\ie the identify and service provider).
The main advantage of this deployment is related to the independency
of each site in case of network disconnections.
% In other words, in case of a network split, each site can satisfy L1 features.
The downside relates to the fact that the current codebase does not
provide any mechanism to allow the collaboration between several
regions and thus L2 requirements cannot be met.~\endnote{Due to space
limitation, we did not discuss a third scenario leveraging the
OpenStack \emph{cells} concept. However, we underline that such an approach
has the drawback of the two discussed scenarios.}


\vspace*{-.1cm}
\subsection{Effective Collaboration is Needed}
Despite the fallibility of the network, and frequent isolation risks
of an \edge site from the rest of the infrastructure, an \edge
infrastructure may be able to meet L3 requirements. This can be
achieved by supposing a collaboration a la \emph{peer-to-peer},
that is, an \edge site always serves local resources and collaborates
with other \edge sites if need be. To develop such a resource
management system, two fundamental design options exist:
\emph{top-down} or \emph{bottom-up}. Both designs differently impact the way
this required collaboration can be handled.

A top-down collaboration design implements the collaboration by federating
VIMs' API. As a consequence the existing VIM codebases are used without introducing
modifications/extensions. Examples of approaches following this
design are: ONAP~\cite{onap}, Kingbird~\cite{kingbird},
FogBow~\cite{brasileiro2016fogbow} and p2p-OpenStack~\cite{ericsson-p2p}.

A bottom-up collaboration design lays on making VIMs
mechanisms/services directly collaborative~\cite{7923796}. For example, having two
OpenStack Nova services able to cooperate and communicate directly
would be a realization of a bottom-up design. Such design implies
either the modification/extension of existing VIMs or the creation of
a completely new system.
