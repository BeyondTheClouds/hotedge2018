
\section{Requirements}
\label{sec:requirements}

\begin{figure}[t]
  \centering
  \def\svgwidth{\columnwidth}
  \input{figures/sites.pdf_tex}
  \caption{Partial representation of the targeted edge infrastructure, composed
    of multiple sites. Each site is composed of many servers (represented by
    black bullets) and have a set of users (depicted by black squares). The red
    dashed line depicts a split-brain situation that separates the
    infrastructure into $2$ partitions, isolating \emph{Site 1} from \emph{Site
    2} and \emph{3}.}
  \label{fig:sites}
\end{figure}

In this study, we target an infrastructure based on hundreds of geo-distributed
edge sites, each of which is composed of at least five servers.
%that might run both control and compute services. (should be said later)
\Cref{fig:sites} gives a simplistic and partial representation of such
infrastructure by depicting only $3$ sites.
% latency between site from ~ms to ~100ms (relevant for the paper?)

\begin{table*}
    \centering
        \input{tab_req.tex}
    \caption{Classification of the requirements to administrate and use edge
    computing infrastructures in $5$ levels.}
    \label{tab:requirements}
\end{table*}

As previously mentioned, administrators and end-users of edge infrastructures
expect to get a set of high level mechanisms whose assembly results in a system
capable of operating a geo-distributed IaaS infrastructure. Those mechanisms
are provided by different services, each of which is in charge of an aspect of
the infrastructure management. In our study, we will only consider the $6$
following primary services, required to operate and use on-demand
resources~\cite{moreno2012csp}:

\begin{itemize}
  \item The Compute manager is in charge of provisioning compute resources
    (\ie virtual/bare-metal machines, containers), and managing their life cycle
    (\eg configuration, scheduling, deployment, suspend/resume and shut down).
  \item The Image manager is in charge of machine and container images that are
    used to boot compute resources.
  \item The Network manager provides connectivity to the infrastructure:
    virtual networks for compute resources and external access for users.
  \item The Storage manager provides persistent storage facilities to
    compute resources.
  \item The Administrative tools provide administrator and user interfaces to
    operate and use the infrastructure.
  \item Finally, the Information manager monitors data of the infrastructure for
    auditing and accounting.
\end{itemize}

In an edge computing infrastructure, servers located at the edge sites can
include both management services, and compute resources.
%Those services enable administrators and developers/end-users to respectively
%operate and use the infrastructure on demand.

Based on the services previously defined, we propose a classification of the
requirements expected by administrators and end-users to operate and use edge
computing infrastructures composed of many sites. Our classification is based
on $5$ \emph{levels}, starting from the easiest aspects (\ie interacting with
a single site -- level 1, or L1) to more complex aspects like managing multiple
sites (L2), up to considering they can be owned by different operators (L5).
\Cref{tab:requirements} summarizes the classification we detail in the
following:

\paragraph{L1: Operate/use any site}
This level contains all the requirements at the scope of a single site of the
infrastructure. The second row of \cref{tab:requirements} lists those
requirements expected from administrators and users. To operate a single site,
like \emph{Site 1} depicted in \cref{fig:sites}, the resource management
services defined previously must be installed (or upgraded) by an
administrator. After what, admins can use those services to manage users,
accesses, flavors (\ie available capacities of compute resources) or quotas,
regarding \emph{Site 1}.
% edge sites are remote & unmanned and should administrated remotely
% tools need to support intermittent network access to the site

Any end-user of the infrastructure expects to be able to provision compute,
network or storage resources on any single site, supposing he is authorized to.
For instance, a user located at \emph{Site 1} should be able to boot a VM in
\emph{Site 2}, using images, flavors or networks defined in \emph{Site 2} -- as
long as the site is reachable.

Furthermore, administrators and users expect metrics to be monitored (\eg
related to resources, users, and projects) from any single site. This is used
for instance for respectively managing quotas and listing existing resources.

\paragraph{L2: Operate/use several sites:}
We now consider in this level that the operations previously defined in L1
cover at least two sites. For instance, an administrator might desire to
configure users access on a per-site basis. Regarding users, they also expect
a collaboration between sites to boot, for instance, a VM on \emph{Site 1},
using an image defined in \emph{Site 2}. To that end both administrators and
users require to collect data that belong to several sites. We define here two
sub-levels as depicted in \cref{tab:requirements}. Such collaboration between
sites can be either explicit (\ie the targeted sites are explicitly specified),
referenced as the sub-level L2.1, or implicit (L2.2). The implicit manner
suggests that policies (\eg performance objectives, energy consumption) and
constraints (\eg affinity, underlying hardware) are provided by admins and users so
that an edge-aware orchestrator takes the right decisions regarding the users'
desiderata and the state of the infrastructure (\eg auto-scaling, relocating
workloads between sites, re-scheduling faulty resources across sites).
% should be elaborated and/or refs should be given here

\paragraph{L3: Robustness wrt split brains:}
In this level, we now consider split brains which correspond to the situation
where the infrastructure is partitioned due to communication failures.
\Cref{fig:sites} depicts such situation where \emph{Site 1} is isolated from
\emph{Site 2} and \emph{3}. In such case, all operations defined in L1 must be
guaranteed for a partition composed of a single site like \emph{Site 1}
(supposing it is reachable). For instance, a user in \emph{Site 1} should be
able to use \emph{Site 1}'s resources despite the split brain. Regarding
partitions containing several sites, L1 and L2 operations must be guaranteed
for any set of sites inside the partition.
% what about quotas? how to manage them in case of split-brain?
% suppose a user has a global quotas on several sites, it is impossible to
% determine its global consumption in case of split-brains?
Two sub-levels are proposed here: L3.1 considers the robustness of already
deployed resources (\eg a user should be able to access a deployed application
despite the split brain, if he can reach the related site), while the
robustness of management services, treated in L3.2, is required for both admins
and users.
% tricky issue: management of split brain between collaborative services in
% different sites

\paragraph{L4: Multiple Cloud environments:}
Since edge infrastructures are dynamic infrastructures where sites can join and
leave, different versions of an IaaS manager can exist on different sites. One
requirement here is to consider L3 requirements between sites operated by
different manager versions (L4.1). Another requirement is to prevent vendor
lock-in between multiple generic IaaS manager technologies (\eg OpenStack,
CloudStack), and container orchestrators like Kubernetes (L4.2).
As a consequence, it is necessary to discover site's capabilities to determine
their compatibilities. For instance, it is not possible to migrate a VM from
\emph{Site 1} to \emph{Site 2} if the latter is managed by a container
orchestrator.

\paragraph{L5: Multiple operators:}
We now consider that sites can be owned by different operators. As depicted in
\cref{tab:requirements}, no requirements is expected by administrators since an
operator should not be able to administrate another operator's sites. However,
operators should be able to collaborate to offer their resources to users.
Specific metrics should be collected at the scope of operators to manage for
instance billing between them.

