
\section{Requirements}
\label{sec:requirements}

\begin{figure}[t]
  \centering
  \def\svgwidth{\columnwidth}
  \input{figures/sites.pdf_tex}
  \caption{Partial representation of the targeted edge infrastructure, composed
    of multiple sites. Each site homes servers (represented by black bullets)
    and have a set of users (depicted by black squares). The red dashed line
    depicts a split-brain situation that separates the infrastructure into $2$
    partitions, where \emph{Site 1} is isolated from \emph{Site 2} and \emph{3}.}
  \label{fig:sites}
\end{figure}

In this study, we target an infrastructure based on hundreds of geo-distributed
edge sites, each of which is composed of at least five servers.
%that might run both control and compute services. (should be said later)
\Cref{fig:sites} gives a simplistic and partial representation of such
infrastructure by depicting only $3$ sites.
% latency between site from ~ms to ~100ms (relevant for the paper?)

\begin{table*}
    \centering
        \input{tab_req.tex}
    \caption{Classification of the requirements to administrate and use edge
    computing infrastructures in $5$ levels.}
    \label{tab:requirements}
\end{table*}

As previously mentioned, administrators and end-users of edge infrastructures
expect to get a set of high level mechanisms whose assembly results in a system
capable of operating a geo-distributed IaaS infrastructure. Those mechanisms
are provided by different services, each of which is in charge of an aspect of
the infrastructure management. In our study, we will only consider the $6$
following primary services, required to operate and use on-demand
resources~\cite{moreno2012csp}:

\begin{itemize}
  \item The Compute manager is in charge of provisioning compute resources
    (\ie virtual/bare-metal machines, containers), and managing their life cycle
    (\eg configuration, scheduling, deployment, suspend/resume and shut down).
  \item The Image manager is in charge of machine and container images that are
    used to boot compute resources.
  \item The Network manager provides connectivity to the infrastructure:
    virtual networks for compute resources and external access for users.
  \item The Storage manager provides persistent storage facilities to
    compute resources.
  \item The Administrative tools provide administrator and user interfaces to
    operate and use the infrastructure.
  \item Finally, the Information manager monitors data of the infrastructure for
    auditing and accounting.
\end{itemize}

In an edge computing infrastructure, servers located at the edge sites can
include both management services, and compute resources.
%Those services enable administrators and developers/end-users to respectively
%operate and use the infrastructure on demand.

Based on the services previously defined, we propose a classification of the
requirements expected by administrators and end-users to operate and use edge
computing infrastructures composed of many sites. Our classification is based
on $5$ \emph{levels}, starting from the easiest aspects (\ie interacting with
a single site -- level 1, or L1) to more complex aspects like managing multiple
sites (L2), up to considering they can be owned by different operators (L5).
\Cref{tab:requirements} summarizes the classification we detail in the
following:

\paragraph{L1: Operate/use any site}
This level contains all the operations related to a single site of the
infrastructure. For instance, to operate \emph{Site 1} depicted in
\cref{fig:sites}, an administrator might desire to install or update a set of
the previously defined resource management services. After what, those
services can be used to manage users, access, flavors (\ie available
capacities of compute resources) or quotas, regarding \emph{Site 1}.
% edge sites are remote & unmanned and should administrated remotely
% tools need to support intermittent network access to the site

Any end-user of the infrastructure should be able to provision compute, network
or storage resources on any single site, supposing he is authorized to. For
instance, a user located at \emph{Site 1} should be able to provision resources
in a site using the site's services, \eg boot a VM in \emph{Site 2}, using
images, flavors or networks defined in \emph{Site 2} -- as long as the site is
reachable.

Furthermore, administrators and users expect to monitor data (\eg related to
resources, users, and projects) from any single site. This is used for instance
for respectively managing quotas and listing existing resources.

\paragraph{L2: Operate/use several sites:}
We now consider in this level that the operations previously defined in L1
cover at least two sites. For instance, an administrator might desire to
configure users access on a per-site basis, so that a specific user of the
infrastructure can boot a VM on \emph{Site 1}, using an image defined in
\emph{Site 2}. To that end both administrators and users require to collect
data that belong to several sites. As depicted in \cref{tab:requirements}, such
collaboration between site can be either handled in an explicit manner (\ie
admins and users specify explicitly which sites are targeted), references as
the sub-level L2.1, or in an implicit manner (L2.2). The implicit manner
suggests that policies and constraints are provided by admins and users so that
a scheduler takes placement decisions regarding the desired actions. This is
used for auto-scaling and autonomous optimization mechanisms.
% should be elaborated and/or refs should be given here

\paragraph{L3: Robustness wrt split brains:}
In this level, we now consider split brains which correspond to the situation
where the infrastructure is partitioned due to communication failures.
\Cref{fig:sites} depicts such situation where \emph{Site 1} is isolated from
\emph{Site 2} and \emph{3}. In such case, all operations defined in L1 must be
guaranteed for a partition composed of a single site like \emph{Site 1}
(supposing it is reachable). For instance, a user in \emph{Site 1} should be
able to use \emph{Site 1}'s resources despite the split brain. Regarding
partitions containing several sites, L1 and L2 operations must be guaranteed
for any set of sites inside the partition.
% what about quotas? how to manage them in case of split-brain?
% suppose a user has a global quotas on several sites, it is impossible to
% determine its global consumption in case of split-brains?
Two sub-levels are proposed here since the robustness can be w.r.t. the
applications deployed on compute resources (L3.1) or to the management services
(L3.2).
% tricky issue: management of split brain between collaborative services in
% different sites

\paragraph{L4: Multiple Cloud environments:}
We consider here L3 mechanisms while being able to manage multiple
infrastructure environments to bring flexibility w.r.t. the
infrastructure managers of the different sites. Since edge infrastructures are
dynamic infrastructures where sites can join and leave, different versions of an
IaaS manager can exist on different sites. One requirement is to make them
collaborate (L4.1). Another requirement is to prevent vendor lock-in between
multiple generic IaaS manager technologies (\eg OpenStack, CloudStack), and
container orchestrators like Kubernetes (L4.2).

\paragraph{L5: Multiple operators:}


