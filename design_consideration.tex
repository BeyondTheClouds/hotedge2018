%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:

%\section{Design Considerations}
%\label{design_considerations}

\section{OpenStack Case Study}
\label{sec:system_design_considerations}
As previously mentioned, the starting point for this study is an edge
infrastructure composed of hundreds of even thousands of micro DCs (\aka edge sites) operated by OpenStack.
%
Similarly to most of resource management systems, an OpenStack system consists of two kinds
of nodes: on the one hand, the compute/storage/network nodes are
dedicated to deliver the XaaS capabilities, such as hosting VMs (\ie
data plane); on the other hand, the control nodes are in charge of
executing the OpenStack services (\ie control plane).
%
Although each edge site provides enough resources to host control and
compute capabilities, several deployments scenarios can be
envisioned.
%
In this section, we discuss two extreme cases. The first one
consists in deploying all control services on one site of the
infrastructure and considering all resources available on the other sites as compute
nodes.  The second one corresponds to a specific multi-region deployment.
%

This preliminary discussion enables us to justify why collaborations is mandatory to
fulfill at least the first three levels of requirements identified in Section~\ref{sec:requirements}.
%
% We emphasize that although OpenStack has been considered, alternative
% systems such as Kubernetes would have been lead to the same conclusion
% in terms of resource management and collaboration needs.  \AL{Put
%   references Kubernetes}

\subsection{Centralized (Remote) Management}
\label{subsec:centralized_os}
At coarse-grained, the first scenario consists in operating an edge
infrastructure like a traditional cloud computing one.  The ``only''
difference is related to the wide-area network links between the
control services and the compute nodes.  The distinction between the
different edge sites can be done by leveraging the concept of host
aggregates (\aka availability zones) provided by OpenStack.

%
Technically speaking, OpenStack services are organized following the
Shared Nothing principle. Each instance of a service (i.e., service
worker) is exposed through an API accessible through a Remote
Procedure Call (RPC) system implemented, on top of a messaging queue
or via web services (REST). This enables a weak coupling between
services and thus a large number of deployment possibilities such as
this one.
%
From the requirements' viewpoint, only L1
and most L2 features~\footnote{Because the infrastructure can
  be spread over several network domains, advanced network operations
  cannot be satisfied without any change in the OpenStack codebase.}
can be fulfilled in a straightforward way.  In addition to
the scalability limitation that will prevent such a deployment to 
consider thousands of compute nodes, it is not possible to satisfy the
L3 expectations.
%
Even if the organization of the OpenStack services respects the Shared
Nothing principle, most services create and manipulate logical objects
that are persisted in shared databases. While this enables service
workers to easily collaborate, it imposes a permanent connectiviy
between compute nodes, services and their DBs. In other words, while
this scenario provides a ``Single Pane of Glass'' from the
administrator and end-users/developers viewpoint, it has the drawback
of being a ``Single Point of Failure'' preventing end-users/developers
to use edge resources in case of network split brains.


\subsection{Regions}
In this second scenario, each edge site corresponds to a a
\emph{Region} in the OpenStack terminology; \ie a complete deployment
of OpenStack including all control services with a ``shared'' Keystone
(\ie the identify and service provider). In our scenario, we consider
that the states of each Keystone are shared through an active/active
prelication scheme leveraging the Galera solution~\cite{www:galera}.
However, alternatives approaches such as federated keystone could have
been considered.  The main advantage of this deployment is related to
the independency of each site in case of network disconnections.  In
other words, in case of a network split, each site can satisfy the L1
features.  The downside is related to the fact that the current
codebase does not provide any mechanism to allow the collaboration
between several regions. In other words, each region can be seen as a
private cloud system and additional mechanisms are required.

